{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segundo ejemplo, ahora con imágenes\n",
    "\n",
    "Construiremos un ejemplo de red neuronal muy simple, con apenas un par de capas, para abordar el famoso problema [MNIST](https://en.wikipedia.org/wiki/MNIST_database). Nuestro objetivo es ver el flujo de trabajo habitual para describir un modelo en Keras, pero sin profundizar en los detalles.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es cargar la librería keras. Podemos ver la versión que tenemos. Recuerda que está habiendo un cambio grande en TensorFlow en su versión 2.0, y en Colab tenemos que forzar el uso de TensorFlow 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "# Descomentar la siguiente línea en Colab\n",
    "# %tensorflow_version 1.x\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparación de los datos\n",
    "\n",
    "El primer paso de todo modelado es la preparación y carga de los datos. En nuestro caso, del problema [MNIST](http://yann.lecun.com/exdb/mnist/), que consta de una gran base de datos de dígitos escritos y que es tan habitual que se ha convertido en un ejemplo paradigmático dentro de Machine Learning.\n",
    "\n",
    "El trabajo de preprocesamiento necesario para poder aplicar un modelo a este problema no es menor pero, afortunadamente, Keras proporciona una instrucción directa para descargar las imágenes que representan los miles de dígitos escritos a mano (ya con formato unificado de 28x28 pixels en escala de grises).\n",
    "\n",
    "Para poder cargar los datos que trae de ejemplo Keras hay que seguir dos pasos: primero, cargar la librería de Keras que proporciona las herramientas para trabajar con el dataset concreto (que suelen estar en el paquete `keras.datasets`, en este caso llamado `mnist`); y, segundo, ejecutar el proceso de carga de los datos (la librería proporciona la función `load_data()`). Ha de tenerse en cuenta que la primera vez que se realiza este proceso los datos se descargan desde un repositorio que viene por defecto predefinido en ese paquete, ya que, debido a su tamaño, no se instalan junto con la librería, sino únicamente cuando el usuario los necesita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que el proceso de carga de datos separa adecuadamente las diversas partes de que consta este dataset:  (_conjunto de entrenamiento_, _conjunto de test_), y cada uno de estos conjuntos está formado por un conjunto de datos (imágenes, en este caso concreto), con sus respectivas etiquetas de clasificación (_labels_). Además, aprovechamos la capacidad sintáctica de Python para realizar la carga de todos estos conjuntos en un solo paso (haciendo una \n",
    "asignación múltiple).\n",
    "\n",
    "Podemos explorar un poco cómo son cada una de estas variables haciendo uso de instrucciones específicas de Python que nos dan información acerca de su estructura y muestra los primeros valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos ver alguna de las imágenes que hay en el dataset, podemos hacer uso de la siguiente función que hace uso de la librería `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gen_image(arr):\n",
    "    conv = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(conv, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    return plt\n",
    "\n",
    "# Dibujamos el primer ejemplo\n",
    "gen_image(test_data[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El flujo de trabajo es similar al que se sigue siempre en los procesos de ML Supervisado, y que hemos analizado en la introducción: \n",
    "  1. Mostramos al modelo (una red neuronal, en nuestro caso) los datos de *entrenamiento*, `train_data` y `train_labels`.\n",
    "  2. El modelo debe *aprender* a asociar las imágenes con las etiquetas asociadas.\n",
    "  3. Por último, verificamos el aprendizaje realizado comprobando sobre `test_data` que las respuestas dadas por el modelo (*predicciones*) coinciden con las almacenadas en `test_labels`.\n",
    "  \n",
    "  <img src=\"https://thumbs.gfycat.com/FickleHorribleBlackfootedferret-small.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definición del modelo\n",
    "\n",
    "Ya estamos en condiciones de definir una red neuronal que consumirá los datos anteriores para ver si somos capaces de dar una primera solución al problema del reconocimiento de dígitos manuscritos. Como solo estamos haciendo una primera aproximación a Keras, la red definida será muy básica, con solo una capa de entrada y una de salida:\n",
    "\n",
    "  1. Vamos a situar una capa de entrada con 784 (= 28 * 28) neuronas (que recibirán cada uno de los 784 pixels de cada imagen), con función de activación ReLU, y \n",
    "  2. una capa de salida con 10 neuronas (una neurona para cada una de las posibles etiquetas de salida), y con activación softmax (por lo que se podrá interpretar como una probabilidad de salida que indica lo probable que es que la imagen de entrada tenga cada una de las etiquetas como salida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "red = models.Sequential()\n",
    "red.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "red.add(layers.Dense(10, activation='softmax'))\n",
    "# plot_model(red, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/miguelamda/DL/master/3.%20Frameworks%20Software/imgs/model_plot.png\" />\n",
    "\n",
    "Para facilitar la comprensión de esta introducción, hemos hecho uso de las utilidades de Keras para dar una representación visual de la estructura de la red. Ha de tenerse en cuenta que para ello es necesario instalar [Graphviz](http://www.graphviz.org) y la librería `pydot` de Python que se comunica con ella.\n",
    "\n",
    "Además de las neuronas, que son las unidades atómicas que componen una red neuronal, desde un punto de vista funcional, el elemento básico de las redes neuronales es lo que se conoce como *capa* (*layer*), un módulo de procesamiento  formado por un conjunto de neuronas iguales que puede ver interpretarse como un \"filtro\" de datos. Como veremos a lo largo del curso, las capas son las encargadas de generar *representaciones* útiles de los datos que reciben, y que ayuden a resolver el problema para el que se ha construido la red. La mayor parte del Deep Learning, y donde esta demostrando un valor añadido respecto de los otros modelos de ML existentes, consiste en concatenar capas simples (y, posiblemente, con funcionalidades específicas diferenciadas) para obtener un dispositivo de cálculo que procesa datos de forma progresiva.\n",
    "\n",
    "En el caso de la red que hemos definido, este dispositivo consta de una secuencia de dos capas densas, que son capas neurales totalmente conectadas. La segunda (y última) capa es una capa \"softmax\" de 10 salidas, lo que significa que devolverá un vector probabilístico de 10 valores (es decir, 10 valores en $[0,1]$ que suman 1). Cada uno de estos valores se interpretará la probabilidad de que la imagen actual pertenezca a una de las 10 clases (los dígitos del 0 al 9).\n",
    "\n",
    "Hasta ahora solo hemos definido la estructura de la red, pero no hemos dado ninguna información acerca de cómo se llevará a cabo el entrenamiento. Para ello, hemos de indicarle a Keras algunas características adicionales, tales como el optimizador que permitirá modificar los pesos de la red, qué función objetivo (de error) se usará para dirigir esta optimización, y la métrica que usaremos para medir cómo se va comportando la red a medida que se entrena. \n",
    "\n",
    "Keras proporciona la función `compile` que permite establecer estas (y otras) propiedades sobre una red ya definida:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "red.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso estamos pasando todos los datos como cadenas que vienen predefinidas en Keras y trabajan con parámetros fijos, pero también es posible ajustar con más flexibilidad cada una de ellas y configurar los parámetros de los que depende, e incluso pasarle funciones, ya sean las que trae Keras o completamente personalizadas. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "red.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.categorical_crossentropy,\n",
    "              metrics=[metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como se puede observar, muchos de los chunks no proporcionan una salida imprimible, sino que crean nuevas variables o modifican el contenido de algunas de ellas para su reutilización posterior.\n",
    "\n",
    "Debido a que la red neuronal que vamos a usar debe recibir como dato de entrada cada imagen de forma aplanada (es decir, no como una matriz de 28x28, sino como un vector de 28x28=784 posiciones), nuestro primer paso es hacer uso de las instrucciones que proporciona Keras para transformar la forma de los datos de entrada. Además, aprovecharemos para normalizar el contenido de estas imágenes (están en escalas de grises con valores `uint8` entre 0 y 255, y las pasaremos a valores `float32` en $[0,1]$), algo aconsejable cuando se trabaja con este tipo de modelos:\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/media-p.slid.es/uploads/970798/images/5546082/ezgif.com-video-to-gif__1_.gif\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reshape((60000, 28 * 28))\n",
    "train_data = train_data.astype('float32') / 255\n",
    "\n",
    "test_data = test_data.reshape((10000, 28 * 28))\n",
    "test_data = test_data.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, vamos a convertir las etiquetas (que vienen en el dataset como valores enteros), en vectores binarios para que se correspondan con la salida que nuestra red puede proporcionar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Proceso de entrenamiento\n",
    "\n",
    "Preparados los datos y definida la red (estructura y funcionalidad), podemos hacer uso de la instrucción `fit` para comenzar el proceso de entrenamiento sobre los datos que tenemos. Esencialmente, hemos de indicar sobre qué datos entrenar (entrada y salidas), cuántas iteraciones (epochs) y con qué tamaño de batch (cada cuántos ejemplos el algoritmos actualiza los pesos).\n",
    "\n",
    "Durante el proceso de entrenamiento, Keras informa de los valores que toma la función objetivo, así como de la/s métrica/s que hemos fijado en la compilación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\miguel\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.2565 - categorical_accuracy: 0.9263\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1053 - categorical_accuracy: 0.9682\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0703 - categorical_accuracy: 0.9785\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0517 - categorical_accuracy: 0.9844\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0384 - categorical_accuracy: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x248781c10f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red.fit(train_data, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos tener en cuenta que los valores mostrados son el error y métricas calculados sobre los propios datos de entrenamiento. Sin embargo, como el objetivo de un modelo de aprendizaje es generalizar bien sobre datos que el proceso de entrenamiento no ha visto anteriormente, necesitamos el conjunto de test para evaluar cómo se comporta la red sobre ejemplos que no ha usado para ajustarse.\n",
    "\n",
    "Sobre los datos de entrenamiento alcanzamos rápidamente una precisión de 0.989 (i.e. 98.9%), pero veamos cómo de bien se comporta con los datos de test (que no ha usado para aprender):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = red.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.9804\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lo normal es que la red se comporte peor en los datos de test que en los datos de entrenamiento, ya que el proceso de entrenamiento consiste precisamente en ajustar los pesos para que el error cometido en estos últimos se minimice. Esta diferencia de comportamiento entre entrenamiento y test se denomina **overfitting** (o **sobreajuste**). En todo caso, con una red tan simple como la que hemos usado se alcanzan cotas de casi el 98% de aciertos.\n",
    "\n",
    "Finalmente, podemos ver las predicciones que hace la red sobre algunos datos del conjunto de test (mostramos también las etiquetas aaociadas a los datos usados, pero ten en cuenta que están en formato binarizado, y el índice 1 corresponde a la etiqueta 0, el índice 2 a la etiqueta 1, etc...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta numérica:    [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "Etiqueta categórica:  7\n",
      "Predicción:           7\n"
     ]
    }
   ],
   "source": [
    "def muestra_imagen_prediccion(i):\n",
    "    # mostrar imagen\n",
    "    gen_image(test_data[i]).show()\n",
    "    # mostrar la etiqueta en modo categórico\n",
    "    print(\"Etiqueta numérica:   \",test_labels[i:(i+1)])\n",
    "    # mostrar la etiqueta en numérico\n",
    "    print(\"Etiqueta categórica: \",np.argmax(test_labels[i:(i+1)]))\n",
    "    # mostrar la predicción de la red\n",
    "    print(\"Predicción:          \",np.argmax(red.predict(test_data[i:(i+1)])))\n",
    "\n",
    "\n",
    "x = 70 # prueba aquí otro número para ver otro ejemplo\n",
    "muestra_imagen_prediccion(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes usar el siguiente código para ver k ejemplos mal predecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta numérica:    [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Etiqueta categórica:  4\n",
      "Predicción:           9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta numérica:    [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "Etiqueta categórica:  4\n",
      "Predicción:           6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABzxJREFUeJzt3X+s1XUdx/FzLsKFBM11ZVrDUTHL1hiJFXW139SqtTEDm6Vt1LRlK5jlxh/92XK1tuyHpaSxNbYyGkM354TR1g9/7AZFJM4YZQsWWJZAJMLlcvrLv+L7Pnm5nHvufT0ef94X33POcM99Nz/3e2h3Op0WMP0NTPYHAHpD7BBC7BBC7BBC7BDivF6+2fKBVf7XP5xj205vap/p5+7sEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEELsEKKnXyXNudEZXtK43b5xfXnt0sFZ5f7eGz9d7udt31nu9A93dgghdgghdgghdgghdgghdgghdgjhnH0K+Mt9i8t95/Bdjdtge2Z57b1HLin3wf2Hy32sXOkn7uwQQuwQQuwQQuwQQuwQQuwQQuwQwjn7FPCdpT8u9+osfcPRBeW1W1ZeU+5je/9Y7kwd7uwQQuwQQuwQQuwQQuwQQuwQQuwQwjl7H9i38U3lfvXskXK/98irG7cHVl1dXju2xzl6Cnd2CCF2CCF2CCF2CCF2CCF2COHorQ/8dPjucp/ZnlHum29a3ri19+wa12di+nFnhxBihxBihxBihxBihxBihxBihxDO2Xvg4BffXu5XzKwfYV322+vL/eKRJxu3TnlldzMWNT8+22q1Wn/74KXjfu1XPnSw3Mf2PT3u1+Z/ubNDCLFDCLFDCLFDCLFDCLFDCLFDCOfsPTA6r967Pa/+/M6hcu+M7m3cjn58WXnt67+wp9xXDj1U7u+f859yr2xdc3657z/5inL/3bHLyn3HPUsat/mPPldee/qJp8p9KnJnhxBihxBihxBihxBihxBihxBihxDtTudsn3j+/y0fWNW7N+sjn3jqQLlfP++Zcl9xzUfL/dnh5mfKH/zqN8prLxyYXe7T1Z2HX1vuW697a7n38z91ve30pvaZfu7ODiHEDiHEDiHEDiHEDiHEDiHEDiE8zz4BDq2tvxd+5dw7urxC/Tz7k+suLvfb3/mTxq3bOfotB95R7r//3uJyn7f/ZLmfS3++rv57u/t9Gxq3z738T+W131rT/G/et1qt1uU3l3NfcmeHEGKHEGKHEGKHEGKHEGKHEI7eJsDx+fWTu92+KrqbvR++a9zXPnaifu8DNy0o94t2Pzbu9z7XLv95vX9lxerG7d13fr+8dteHvl3u77n51nIfWt9/f2/u7BBC7BBC7BBC7BBC7BBC7BBC7BDCOfs0MHLijN8c3Gq1Wq0v31o/izln98hEf5y+MfeJfzRu3X7/4G2Ds8r9X1edKveh9eU8KdzZIYTYIYTYIYTYIYTYIYTYIYTYIYRz9ilg54l6/+T9tzRui7Y8PsGfZuoY2/d049bt9w+2d3ne/eEP1F8P/vnWcLlPBnd2CCF2CCF2CCF2CCF2CCF2CCF2COGcfQq47UvN5+itVqu1aHPuWfp4zd3z7GR/hJ5zZ4cQYocQYocQYocQYocQYocQYocQztmngAt2PVPu9TeYcyajl1w42R+h59zZIYTYIYTYIYTYIYTYIYTYIYSjN6at9uBg43Zk3bGzeu2PPP7Zcl/Y2n1Wr38uuLNDCLFDCLFDCLFDCLFDCLFDCLFDCOfsE2Dhg8fL/bkbXyj3iwZml/u+T11a7ou+2/z+pw7Vj8dOZ2NveUPj9siSe8pr/3qq/m+68Jvj+kiTyp0dQogdQogdQogdQogdQogdQogdQrQ7nU7P3mz5wKrevVkfObTlinLf8eaNZ/X6G44uaNzu2LiivPayr+8o987oyXF9ponQnjmr/gNLXlfO1/5oe+O2+oL95bVv/PXqcl/4sf57Xv1F205vap/p5+7sEELsEELsEELsEELsEELsEELsEMI5ew+MvevKcv/72vrZ6Yev/EG5D82Y85I/04uW/uaGcv/3P88f92t3M/8XM8v9+LWHy31nl99P2H1yrHG7YcPa8toF258v9/Yju8p9Mjlnh3BihxBihxBihxBihxBihxCO3qaAzvCScj9024nG7VdX/bC89mXtLo+R9rFfvlB/9jXrP9O4veprj070x+kbjt4gnNghhNghhNghhNghhNghhNghhHP26W7Z4nI+uG603Ls9Rno2th6vH59du7n+OufX/OxY/QYjf3ipH2lacM4O4cQOIcQOIcQOIcQOIcQOIcQOIZyzwzTjnB3CiR1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CiB1CtDudzmR/BqAH3NkhhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghhNghxH8BI4EZpmucgngAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta numérica:    [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "Etiqueta categórica:  6\n",
      "Predicción:           0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta numérica:    [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Etiqueta categórica:  2\n",
      "Predicción:           7\n"
     ]
    }
   ],
   "source": [
    "k=4\n",
    "for i in range(0,len(test_data)):\n",
    "    if np.argmax(red.predict(test_data[i:(i+1)])) != np.argmax(test_labels[i:(i+1)]):\n",
    "        k=k-1\n",
    "        if k<0:\n",
    "            break;\n",
    "        muestra_imagen_prediccion(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
